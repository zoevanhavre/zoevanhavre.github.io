---
title: 'Wombat - 2016 '
author: "Dr Zoe van Havre"
output:
  ioslides_presentation:
    css: custom.css
    incremental: yes
    keep_md: yes
    logo: Images/logo.png
    transition: faster
  beamer_presentation:
    incremental: yes
    slide_level: 2
    toc: yes
  slidy_presentation:
    incremental: yes
subtitle: 'Simple tools for complex problems: making molehills out of mountains'
fontsize: 10pt
---


```{r, echo=FALSE, include=FALSE, cache=TRUE}
# OPTIONS
require(knitr)
# Set opts_knit
opts_knit$set(message=FALSE)
# Set opts_chunk
opts_chunk$set(fig.width=7, fig.height=5, pointsize=8)
load("~/Google Drive/Work/CSIRO_P1/WOMBAT Talk 2016/Results_All_ForPres.RDATA")
```



```{r,echo=FALSE,cache=TRUE}
require(ggplot2, quietly=TRUE)
require(tidyr, quietly=TRUE)

# function:
DensityPlot_HC<-function(ME, BigY_HC_inFunk=BigY_HC, 
                            .All.tp_inFunk=.All.tp){ 
	  y.now=BigY_HC_inFunk[,ME]
    region.now=names(BigY_HC_inFunk)[ME]
    
    # pars:
    pars.now    <-aggregate( Estimate~ variable+k,
      data=.All.tp_inFunk[ .All.tp_inFunk$Region %in% region.now,],
      mean)

    Mix.pars<-list(  "Mu" =c(pars.now[pars.now$variable%in%"Mu",3]),
                     "Var"=c(pars.now[pars.now$variable%in%"Sig",3]),
                     "P"  =c(pars.now[pars.now$variable%in%"P",3]))
  	In <-list("Y"=y.now, "mu"=Mix.pars[[1]], "sigma"=Mix.pars[[2]], "lambda"=Mix.pars[[3]])
    x     <- seq(0.5,4,len=1000)
  	pars    <- data.frame(comp=paste("K",c(1:length(In$lambda)), sep="_"), In$mu, In$sig, In$lambda )
  	em.df   <- data.frame(x=rep(x,each=nrow(pars)),pars)
  	em.df$y <- with(em.df,In.lambda*dnorm(x,mean=In.mu,sd=sqrt(In.sig)))
  	em.df$In.mu<-	factor(em.df$In.mu)
  	
    if(sum(pars.now$k==2)==3) {levels(em.df$In.mu)<-c("HC_k_1", "HC_k_2")
    }else{  levels(em.df$In.mu)<-c("HC_k_1")}

		#model found
		md<-ggplot(data.frame(x=In$Y),aes(x,y=..density..)) + xlab("SUVR")+ylab("Density")+
        geom_histogram(fill="deepskyblue4", alpha = 0.8, binwidth=diff(range(y.now))/50)+
		    geom_polygon(data=em.df,aes(x,y,fill=comp),color="black", alpha=0.5, size=0.2)+
		    scale_fill_manual("Cluster",labels=format(em.df$In.mu,digits=3), values=c( "dodgerblue4", "cyan2"))+ 
		    theme_bw()+
#		  geom_vline(data=pars, aes(xintercept=In.mu),color="black",linetype="dashed", size=0.3)+
#		    theme(legend.position="none")+
#		    annotate("text", x = pars$In.mu[1]-0.01, y =-0.25, label = round(pars$In.mu[1], 2), angle=90, size=2)+ 
		ggtitle(bquote(atop(.(region.now), atop(italic("HC (Blue)")))))+
			 coord_cartesian(ylim= c(-0.5,  max(em.df$y)+0.5))
		
	 return(md)}



# function:
DensityPlot_Both<-function(ME, BigY_HC_inFunk=BigY_HC, 
                            BigY_AD_inFunk=BigY_AD, 
                            .All.tp_inFunk=.All.tp ,
                            .All_AD_inFunk=.All_AD){ 
	  y.now=BigY_HC_inFunk[,ME]
    region.now=names(BigY_HC_inFunk)[ME]
    y.ad.now=BigY_AD_inFunk[,ME]
    
    # pars:
    pars.now    <-aggregate( Estimate~ variable+k,
      data=.All.tp_inFunk[ .All.tp_inFunk$Region %in% region.now,],
      mean)
    .All_AD.tp  <-.All_AD_inFunk[.All_AD_inFunk$TopModel %in% TRUE,]
    pars.now.ad <-aggregate(Estimate~ variable+k,
                           data=.All_AD.tp[ .All_AD.tp$Region %in% region.now,],
                           mean)

    Mix.pars<-list(  "Mu" =c(pars.now.ad[2,3], pars.now[pars.now$variable%in%"Mu",3]),
                     "Var"=c(pars.now.ad[3,3], pars.now[pars.now$variable%in%"Sig",3]),
                     "P"  =c(pars.now.ad[1,3], pars.now[pars.now$variable%in%"P",3]))
  	In <-list("Y"=y.now, "mu"=Mix.pars[[1]], "sigma"=Mix.pars[[2]], "lambda"=Mix.pars[[3]])
    x     <- seq(0.5,4,len=1000)
  	pars    <- data.frame(comp=paste("K",c(1:length(In$lambda)), sep="_"), In$mu, In$sig, In$lambda )
  	em.df   <- data.frame(x=rep(x,each=nrow(pars)),pars)
  	em.df$y <- with(em.df,In.lambda*dnorm(x,mean=In.mu,sd=sqrt(In.sig)))
  	em.df$In.mu<-	factor(em.df$In.mu)
  	
    if(sum(pars.now$k==2)==3) {levels(em.df$In.mu)<-c("HC_k_1", "HC_k_2", "AD_k_1")
    }else{  levels(em.df$In.mu)<-c("HC_k_1", "AD_k_1")}

		#model found
		md<-ggplot(data.frame(x=In$Y),aes(x,y=..density..)) + xlab("SUVR")+ylab("Density")+
		   	geom_histogram( data=data.frame(xad=y.ad.now), aes(xad,y=..density..), fill="firebrick1", alpha = 0.3,binwidth=diff(range(y.ad.now))/50)+
        geom_histogram(fill="deepskyblue4", alpha = 0.8, binwidth=diff(range(y.now))/50)+
		    geom_polygon(data=em.df,aes(x,y,fill=comp),color="black", alpha=0.5, size=0.2)+
		    scale_fill_manual("Cluster",labels=format(em.df$In.mu,digits=3), values=c("firebrick1", "dodgerblue4", "cyan2"))+ 
		    theme_bw()+
#		  geom_vline(data=pars, aes(xintercept=In.mu),color="black",linetype="dashed", size=0.3)+
#		    theme(legend.position="none")+
#		    annotate("text", x = pars$In.mu[1]-0.01, y =-0.25, label = round(pars$In.mu[1], 2), angle=90, size=2)+ 
		ggtitle(bquote(atop(.(region.now), atop(italic("HC (Blue) & AD (Red)")))))+
			 coord_cartesian(ylim= c(-0.5,  max(em.df$y)+0.5))
		
	 return(md)}


```


## Who am I?

- PhD in statistics, from QUT \& Paris-Dauphine
- I live in Brisbane, by way of Canada, New Zealand, and various places in between.
- *Key areas*:
    - <img style="width: 18px; height: 18px; margin: 0; vertical-align: center;" src="http://i.stack.imgur.com/DSxUV.png" alt="" scale="0"> Bayesian statistics
    - Mixture and hidden Markov models, 
    - Bio-statistics/informatics/security,
- *Research interests*
    - data driven, accessible, intuitive tools
    - **making data analysis easier**


## What drives me?

The most common question asked since I started to pursue Statistics has been

<div class="centered">
**"Why...?"**
</div>

- I have three reasons:

1. A sense of urgency,
2. tantalizing hope,
3. boundless excitement.



# Urgency?

## Can we keep up?

- The exponetial growth of computing has not slowed down.
- New types of data and new challenges require new approaches
- 

----------------




# Hope...


## Not all change is bad
- Everyone is coming onboard! amazing advances
- Data-science is a thing now
- We are standing on a methodological goldmine...
    - **the traditional way**: 
        - Develop methods based on large sample theory.  
        - Adapt / make assumptions. to deploy on realistic sample sizes
    - **the future?** 
        - Rework common tools to be closer to underlying theory
        - This usually means Bayesian, yes. Sorry. 
        - Asymptotic theory $\rightarrow$ Methods $\rightarrow$ Big Data  $\rightarrow$ Theoretically Supported Results
    
    
## 3. Excitement!

Amazing things happen when data analysis combines

- clear research questions, 
- suitable tools, and 
- appropriate data



## A short story | overfitted mixtures and Alzheimer's Disease


## Key background



### What you need to know

- Alzheimer's Disease (AD) is something we need to address
- disease development is very slow
- no cognitive changes for $\geq$ 20 years
- once changes evident, extensive physical damage
    - Amyloid $\beta$ deposits
    - Neuro tangles
    - Atrophy
- Tests which assess physical change are $ $ $ and intrusive


## How can we help improve early detection

To better tackle AD, we need to be able to treat it earlier.

- we know little about how AD behaves in its early stage
- could compare known cases to controls, 
    - does not target early stage of AD
- **would like to identify individuals likely to be in early stage of AD**
 
## How? 

- large repository of data exists thanks to AIBL study 
- many data types, potential variables, time points, and sources
- possibilites = *endless* (thousands of potential approaches)
- What now...?




--------------

```{r, echo=FALSE, warning=FALSE,, fig.width = 4 , fig.height = 3}
require(ggplot2, quietly=TRUE)
DensityPlot_HC(5)
DensityPlot_HC(10)
DensityPlot_HC(15)
DensityPlot_HC(29)

```

-----------

```{r, echo=FALSE, warning=FALSE,, fig.width = 4 , fig.height = 3}
require(ggplot2, quietly=TRUE)

DensityPlot_Both(5)
DensityPlot_Both(10)
DensityPlot_Both(15)
DensityPlot_Both(29)

```

-----------

```{r, echo=FALSE,warning=FALSE, fig.height = 6, fig.width = 8}
require(tidyr, quietly=TRUE)
heatmap.3(t(ForSIMI_num),col=c( "dodgerblue4", "cyan2"),hclust=hclustfunc, distfun=distfunc,   key=FALSE, margins = c(2,8),cexRow = .6,cexCol = 0.001, xlab="Patients (HC only)")

```


-------

```{r, echo=FALSE,warning=FALSE, fig.height = 6, fig.width = 8}
require(tidyr, quietly=TRUE)

df.new<-aggregate(ApoE~ Region+k_hilo, droplevels(top_demogs),function(x) prop.table(table(x)) )
df.new<-as.data.frame(as.matrix(df.new))
df.new.t<-gather(df.new,key=Genotype, value=Proportion,ApoE.ApoE_Neg,  ApoE.ApoE_Pos  )
df.new.t$Proportion<- factor2numeric( df.new.t$Proportion)
levels(df.new.t$k_hilo)<-c("HC: high mean group","HC: low mean group")
ggplot(df.new.t,aes(Region,Proportion))+geom_bar(aes(fill=Genotype),stat="identity",position='dodge')+facet_grid(~k_hilo)+theme_bw() +  theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.text.y = element_text( hjust = 1))+ggtitle("Genotype by Zmix cluster")+ scale_fill_manual("Genotype",labels=c("ApoE- (good)", "ApoE+ (bad)"), values=c("red", "yellow"))  


```

-------------


```{r, echo=FALSE,warning=FALSE}
require(tidyr, quietly=TRUE)

# memory status
df.new<-aggregate(Austin~ Region+k_hilo, droplevels(top_demogs),function(x) prop.table(table(x)) )
df.new<-as.data.frame(as.matrix(df.new))
df.new.t<-gather(df.new,key=Diagnosis, value=Proportion,3,   4  )
df.new.t$Proportion<- factor2numeric( df.new.t$Proportion)
levels(df.new.t$Diagnosis)<-c("Memory Complainer (HC)"   ,  "Non-Memory Complainer (HC)")
levels(df.new.t$k_hilo)<-c("HC: high mean group","HC: low mean group")
ggplot(df.new.t,aes(Region,Proportion))+geom_bar(aes(fill=Diagnosis),stat="identity",position='dodge')+facet_grid(~k_hilo)+theme_bw() +  theme(axis.text.x = element_text(angle = 90, hjust = 1), axis.text.y = element_text( hjust = 1))+ggtitle("Memory Status by Zmix cluster")+ scale_fill_manual("Memory",labels=c("Complainer", "Non-Complainer"), values=c("orange", "green"))  
  
```




## Slide with image
<img src="Images/hc_diff_means.png" style="width: 800px"/>


